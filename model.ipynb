{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing of images:\n",
      "Retrieving images...\n",
      "0/7200 [>-----------------------------] - ETA: 0:00\n",
      "Done Retrieving images!\n",
      "Mirroring images...\n",
      "\r"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mget_images\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_data\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Load the dataset\u001B[39;00m\n\u001B[1;32m      7\u001B[0m (train_images, train_labels), (test_images, test_labels), identity_mapper, size_of_dense_layer \\\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;241m=\u001B[39m \u001B[43mget_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_faces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_unknown\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpercent_training_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistribution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meven\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                          \u001B[49m\u001B[43madd_mirrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_rotations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Cal Poly/CSC 487/csc487_project/get_images.py:267\u001B[0m, in \u001B[0;36mget_data\u001B[0;34m(num_faces, num_unknown, percent_training_data, distribution, add_mirrors, num_rotations)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m do_process_photo, t1, t_orig, total_photos, total_processed, total_people_processed, j\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m add_mirrors \u001B[38;5;129;01mor\u001B[39;00m (num_rotations \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 267\u001B[0m     \u001B[43maugment_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_mirrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_rotations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# augment_data(training_images, training_labels, add_mirrors, num_rotations)\u001B[39;00m\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;66;03m# This line adds data augmentations to training data too, which we may or may not want to do\u001B[39;00m\n\u001B[1;32m    271\u001B[0m test_images, test_labels \u001B[38;5;241m=\u001B[39m shuffle_arrays(test_images, test_labels)\n",
      "File \u001B[0;32m~/Desktop/Cal Poly/CSC 487/csc487_project/get_images.py:104\u001B[0m, in \u001B[0;36maugment_data\u001B[0;34m(images, labels, mirror, num_rotate)\u001B[0m\n\u001B[1;32m    101\u001B[0m     sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    102\u001B[0m     sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mstr\u001B[39m(total_processed) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(total_photos) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 104\u001B[0m           \u001B[38;5;241m+\u001B[39m \u001B[43mget_progressbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_processed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_photos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    105\u001B[0m           \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m - ETA: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m get_minutes_from_seconds(\u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDone mirroring images!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_rotate \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/Cal Poly/CSC 487/csc487_project/get_images.py:18\u001B[0m, in \u001B[0;36mget_progressbar\u001B[0;34m(current, total)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_progressbar\u001B[39m(current, total):\n\u001B[0;32m---> 18\u001B[0m     progress \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mround\u001B[39m((\u001B[43mcurrent\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43mtotal\u001B[49m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m30\u001B[39m)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m progress \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m30\u001B[39m:\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[==============================]\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from get_images import get_data\n",
    "\n",
    "# Set variables\n",
    "NUM_IDENTITIES = 100\n",
    "\n",
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels), identity_mapper, size_of_dense_layer \\\n",
    "    = get_data(num_faces=NUM_IDENTITIES, num_unknown=0, percent_training_data=15.0, distribution=\"even\",\n",
    "                          add_mirrors=True, num_rotations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_images, test_images \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_images\u001B[49m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m, test_images \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "print(test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((1, 1)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(NUM_IDENTITIES))\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=10,\n",
    "                    validation_data=(test_images, test_labels))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss with Modified Data\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
